name: RL Service Testing Pipeline

on:
  push:
    branches: [ main, develop, feature/* ]
    paths:
      - 'backend/rl-service/**'
      - '.github/workflows/rl-service-testing.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/rl-service/**'
  schedule:
    # Run full test suite daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'full'
        type: choice
        options:
          - smoke
          - unit
          - integration
          - performance
          - full

env:
  PYTHON_VERSION: '3.11'
  RL_ENVIRONMENT: 'testing'
  RL_DEBUG: 'true'
  RL_LOG_LEVEL: 'INFO'

jobs:
  # Job 1: Code Quality and Security Checks
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Code formatting check (Black)
        run: |
          cd backend/rl-service
          black --check --diff .
          
      - name: Import sorting check (isort)
        run: |
          cd backend/rl-service
          isort --check-only --diff .
          
      - name: Linting (flake8)
        run: |
          cd backend/rl-service
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          
      - name: Type checking (mypy)
        run: |
          cd backend/rl-service
          mypy . --ignore-missing-imports --no-strict-optional
          
      - name: Security check (bandit)
        run: |
          cd backend/rl-service
          bandit -r . -f json -o security-report.json || true
          
      - name: Dependency vulnerability scan (safety)
        run: |
          cd backend/rl-service
          safety check --json --output safety-report.json || true
          
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            backend/rl-service/security-report.json
            backend/rl-service/safety-report.json

  # Job 2: Smoke Tests (Fast validation)
  smoke-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    if: github.event.inputs.test_level == 'smoke' || github.event.inputs.test_level == 'full' || github.event.inputs.test_level == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Run smoke tests
        run: |
          cd backend/rl-service
          pytest -m smoke --tb=short --maxfail=5 -v
          
      - name: Upload smoke test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: smoke-test-results
          path: backend/rl-service/test_reports/

  # Job 3: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    needs: smoke-tests
    if: github.event.inputs.test_level == 'unit' || github.event.inputs.test_level == 'full' || github.event.inputs.test_level == ''
    strategy:
      matrix:
        test-group: [environment, agents, rewards, integration]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Run unit tests for ${{ matrix.test-group }}
        run: |
          cd backend/rl-service
          pytest tests/test_*${{ matrix.test-group }}* -m unit --cov --cov-report=xml --tb=short -v
          
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: backend/rl-service/coverage_reports/coverage.xml
          flags: unit-tests-${{ matrix.test-group }}
          name: codecov-${{ matrix.test-group }}

  # Job 4: Integration Tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event.inputs.test_level == 'integration' || github.event.inputs.test_level == 'full' || github.event.inputs.test_level == ''
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: rl_trading_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
          
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Setup test database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/rl_trading_test
        run: |
          cd backend/rl-service
          python -c "
          import sqlalchemy
          engine = sqlalchemy.create_engine('$DATABASE_URL')
          # Initialize test database schema
          print('Database connection successful')
          "
          
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/rl_trading_test
          REDIS_URL: redis://localhost:6379
        run: |
          cd backend/rl-service
          pytest tests/test_integration.py -m integration --cov --cov-report=xml --tb=short -v --timeout=300
          
      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            backend/rl-service/test_reports/
            backend/rl-service/coverage_reports/

  # Job 5: Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event.inputs.test_level == 'performance' || github.event.inputs.test_level == 'full' || github.event.inputs.test_level == ''
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Run performance validation tests
        run: |
          cd backend/rl-service
          pytest tests/test_performance.py -m performance --tb=short -v --timeout=3600 --benchmark-only --benchmark-json=benchmark_results.json
          
      - name: Run backtesting validation
        run: |
          cd backend/rl-service
          pytest tests/test_backtesting.py --tb=short -v --timeout=2400
          
      - name: Upload performance test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            backend/rl-service/test_reports/
            backend/rl-service/benchmark_results.json

  # Job 6: Stress Tests
  stress-tests:
    runs-on: ubuntu-latest
    needs: performance-tests
    if: github.event.inputs.test_level == 'full' || github.event_name == 'schedule'
    timeout-minutes: 45
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Run stress tests
        run: |
          cd backend/rl-service
          pytest tests/stress_testing.py -m stress --tb=short -v --timeout=2700
          
      - name: Upload stress test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: stress-test-results
          path: backend/rl-service/test_reports/

  # Job 7: Benchmark Comparison
  benchmark-tests:
    runs-on: ubuntu-latest
    needs: performance-tests
    if: github.event.inputs.test_level == 'full' || github.event_name == 'schedule'
    timeout-minutes: 90
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          # Also install ml-service dependencies for AdaptiveThreshold comparison
          pip install -r ../ml-service/requirements.txt
          
      - name: Run benchmark comparison tests
        run: |
          cd backend/rl-service
          pytest tests/benchmark_comparison.py -m benchmark --tb=short -v --timeout=5400
          
      - name: Upload benchmark comparison results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-test-results
          path: backend/rl-service/test_reports/

  # Job 8: Generate Test Reports
  generate-reports:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          cd backend/rl-service
          pip install --upgrade pip
          pip install -r requirements-test.txt
          pip install -r requirements.txt
          
      - name: Download all test artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts/
          
      - name: Generate comprehensive test report
        run: |
          cd backend/rl-service
          python tests/test_report_generator.py
          
      - name: Upload comprehensive test report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: comprehensive-test-report
          path: |
            backend/rl-service/test_reports/
            artifacts/

  # Job 9: SOW Compliance Check
  sow-compliance:
    runs-on: ubuntu-latest
    needs: [performance-tests, benchmark-tests]
    if: always() && (github.event.inputs.test_level == 'full' || github.event_name == 'schedule' || github.ref == 'refs/heads/main')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download test results
        uses: actions/download-artifact@v3
        with:
          name: performance-test-results
          path: performance-results/
          
      - name: Download benchmark results
        uses: actions/download-artifact@v3
        with:
          name: benchmark-test-results
          path: benchmark-results/
          
      - name: SOW Compliance Validation
        run: |
          echo "=== SOW COMPLIANCE CHECK ==="
          echo "Validating against SOW requirements:"
          echo "- 3-5% weekly returns"
          echo "- Sharpe ratio > 1.5"
          echo "- Maximum drawdown < 15%"
          echo "- Win rate > 60%"
          echo "- 15-20% outperformance vs AdaptiveThreshold"
          
          # This would typically parse test results and validate compliance
          # For now, we'll create a compliance summary
          
          COMPLIANCE_STATUS="PENDING"
          if [ -f "performance-results/junit.xml" ] && [ -f "benchmark-results/junit.xml" ]; then
            COMPLIANCE_STATUS="REQUIRES_REVIEW"
          fi
          
          echo "COMPLIANCE_STATUS=$COMPLIANCE_STATUS" >> $GITHUB_ENV
          
      - name: Create compliance report
        run: |
          cat > sow_compliance_report.md << EOF
          # SOW Compliance Report
          
          **Date:** $(date)
          **Branch:** ${{ github.ref }}
          **Commit:** ${{ github.sha }}
          **Status:** ${{ env.COMPLIANCE_STATUS }}
          
          ## Performance Targets
          - [ ] Weekly returns: 3-5%
          - [ ] Sharpe ratio: >1.5
          - [ ] Maximum drawdown: <15%
          - [ ] Win rate: >60%
          - [ ] Outperformance vs baseline: 15-20%
          
          ## Test Results Summary
          - Unit Tests: See artifacts
          - Integration Tests: See artifacts  
          - Performance Tests: See artifacts
          - Benchmark Tests: See artifacts
          
          **Note:** Detailed compliance validation requires manual review of test results.
          EOF
          
      - name: Upload SOW compliance report
        uses: actions/upload-artifact@v3
        with:
          name: sow-compliance-report
          path: sow_compliance_report.md

  # Job 10: Notify on Critical Failures
  notify-failures:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, sow-compliance]
    if: failure() && (github.ref == 'refs/heads/main' || github.event_name == 'schedule')
    
    steps:
      - name: Notify on critical test failures
        uses: actions/github-script@v6
        with:
          script: |
            const issue_body = `
            ## Critical Test Failure Alert
            
            **Workflow:** ${{ github.workflow }}
            **Branch:** ${{ github.ref }}
            **Commit:** ${{ github.sha }}
            **Run:** ${{ github.run_id }}
            
            Critical tests have failed in the RL Trading System. Please investigate immediately.
            
            **Failed Jobs:**
            ${{ toJSON(needs) }}
            
            **Action Required:**
            1. Review test failures
            2. Check SOW compliance
            3. Verify system performance
            4. Fix critical issues before deployment
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🚨 Critical RL System Test Failure',
              body: issue_body,
              labels: ['critical', 'testing', 'rl-service']
            });

# Reusable workflow for manual test execution
workflow_call:
  inputs:
    test_type:
      required: true
      type: string
    timeout_minutes:
      required: false
      type: number
      default: 30